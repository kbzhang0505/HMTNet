# HMTNet
After the paper is accepted, we will provide code for the paper: HMTNet: A Hierarchical Multi-Task Network for Super-Resolution Image Quality Assessment.
# Abstract
Super-resolution (SR) image quality assessment (SR-IQA) plays a crucial role in evaluating the performance and optimizing model parameters of SR algorithms. However, most existing SR-IQA methods typically quantify the degradation of SR images through a single-task regression framework to map visual perceptual features to quality scores. Although these methods perform well, they may exhibit bias when evaluating certain SR images. In this paper, we propose a novel hierarchical multi-task framework, called HMTNet, for no-reference super resolution image quality assessment (NR-SRIQA), to measure the quality of SR images in a coarse-to-fine manner. The newly proposed HMTNet first employs a quality grading network which integrates a U-Net and contrastive learning, to divide image quality into five grades coarsely, as the quality grading prior knowledge to conducting qualitative evaluation of SR image quality. Then a triple-perspective Shared Feature Extractor Network (SFEN) that integrates texture details, gradient and saliency features, is developed to build a fine mapping from visual perception features to quantitative quality score guided by the learned quality grading prior knowledge. Under the coarse-to- fine SR-IQA framework, the proposed HMTNet can quantify the degradation of SR images more accurately and comprehensively. Thorough experiments on both QADS and MA datasets indicate that our HMTNet achieves better human visual consistency compared to other state-of-the-art SR-IQA methods.
# The overall framework
![HMTNet_1](https://github.com/user-attachments/assets/06bc48ab-b8a7-4ed1-aa96-451f07dfe471)
# Quantitative results
![HMTNet_2](https://github.com/user-attachments/assets/78f25b71-9ce1-4b57-8803-7a1fca69eb78)
# Qualitive results
![1](https://github.com/user-attachments/assets/93ae3c03-49dc-4ebb-a1e3-dc866d310cad)
Fig. 8 Eight best cases are derived from the results of the evaluation of SR images using the proposed method on MA database. The left / middle / right values denote MOS value, the prediction score by our method, and the prediction score by C2MT, respectively. The third row represents the predicted quality grading results of SR images.
![2](https://github.com/user-attachments/assets/45285544-0cfc-4b55-9790-357df5f15e2d)
Fig. 9 Four worst cases are derived from the results of the evaluation of SR images using the proposed method on MA database. The left / middle / right values denote MOS value, the prediction score using our method, and the prediction score using C2MT. The third row represents the predicted quality grading results of SR images.
